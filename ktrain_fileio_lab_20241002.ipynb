{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chani1352/python/blob/main/ktrain_fileio_lab_20241002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab"
      ],
      "metadata": {
        "id": "ZYJeA4fpcGNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. 미국 대통령에 관한 정보를 담은 파일을 읽고 이를 처리하는 프로그램을 작성하세요. 그로버 클리블랜드처럼 비연속적인 임기를 지낸 대통령들은 각각의 임기가 따로 나열되어 있습니다.\n",
        "\n",
        "* President 클래스를 작성하고, 대통령의 이름, 각 임기의 시작 및 종료 연도를 추출하는 메서드를 작성하세요. 또한, 대통령이 총 4년 이상을 재임했는지(연속적이든 아니든)를 확인하는 메서드를 작성하세요.\n",
        "\n",
        "* 현재 재임 중인 대통령(예: 조 바이든, 종료 연도가 없는 경우)의 처리를 포함해야 합니다.\n",
        "\n",
        "* 파일: https://github.com/dbwebb-se/vlinux/blob/master/example/grep/presidents.txt\n",
        "\n",
        "* 파일의 각 줄에는 대통령의 이름과 그들이 재임한 연도가 쉼표로 구분되어 나열되어 있습니다.\n",
        "\n",
        "### President 클래스\n",
        "\n",
        "* 생성자 (__init__): President 객체를 초기화하며 대통령의 이름을 저장하고 process_term 메서드를 사용하여 각 임기를 처리합니다. 임기의 목록은 문자열로 전달되며, 각 문자열은 \"1789-1797\" 또는 \"1885-1889, 1893-1897\"처럼 표현됩니다.\n",
        "\n",
        "> 힌트: 생성자에서 list comprehension을 사용하여 각 임기 문자열을 __process_term 메서드를 사용해 (start_year, end_year) 튜플로 변환하세요. 이렇게 하면 대통령의 임기 데이터를 쉽게 접근하고 조작할 수 있습니다.\n",
        "\n",
        "* __process_term 메서드: 대통령 임기를 나타내는 문자열(예: \"1789-1797\")을 받아 시작 및 종료 연도를 정수로 변환하는 튜플로 바꿉니다. 종료 연도가 없으면(예: 현재 대통령인 경우) 현재 연도를 할당합니다.\n",
        "\n",
        "> 힌트: 문자열 분할(term.split('-'))을 사용해 임기를 시작 연도와 종료 연도로 나누세요. 종료 연도가 없으면 datetime.now().year를 사용하여 현재 연도를 할당하세요. 이 메서드는 완료된 임기와 진행 중인 임기 모두를 처리할 수 있도록 합니다.\n",
        "\n",
        "* is_served_multiple 메서드: 대통령이 총 4년 이상 재임했는지 또는 여러 임기를 지냈는지 확인합니다. 모든 임기를 합산하여 총 재임 기간이 4년을 초과하는지 확인합니다.\n",
        "\n",
        "> 힌트: generator expression을 사용해 각 임기의 종료 연도와 시작 연도 사이의 차이를 합산하여 총 재임 기간을 계산하세요.\n",
        "\n",
        "### 함수\n",
        "\n",
        "read_presidents_from_file 함수: 파일에서 각 줄에 대통령의 이름과 재임 기간이 포함된 데이터를 읽고, President 객체의 목록을 생성합니다.\n",
        "\n",
        "> 힌트: split(', ', 1)을 사용해 이름과 임기를 분리하세요. 그런 다음 임기를 목록으로 나누고 President 생성자에 전달하세요. 파일이 없는 경우 try-except 블록을 사용하여 예외 처리를 하세요.\n"
      ],
      "metadata": {
        "id": "0AZqhmeccW7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = open('./presidents.txt')\n",
        "for line in file_content:\n",
        "  print(line.rstrip())"
      ],
      "metadata": {
        "id": "i2Oj7De4uVlO",
        "outputId": "f38ae795-bb9b-4993-812f-d404b861abd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George Washington, 1789-1797\n",
            "John Adams, 1797-1801\n",
            "Thomas Jefferson, 1801-1809\n",
            "James Madison, 1809-1817\n",
            "James Monroe, 1817-1825\n",
            "John Quincy Adams, 1825-1829\n",
            "Andrew Jackson, 1829-1837\n",
            "Martin Van Buren, 1837-1841\n",
            "William Henry Harrison, 1841-1841\n",
            "John Tyler, 1841-1845\n",
            "James Knox Polk, 1845-1849\n",
            "Zachary Taylor, 1849-1850\n",
            "Millard Fillmore, 1850-1853\n",
            "Franklin Pierce, 1853-1857\n",
            "James Buchanan, 1857-1861\n",
            "Abraham Lincoln, 1861-1865\n",
            "Andrew Johnson, 1865-1869\n",
            "Ulysses S. Grant, 1869-1877\n",
            "Rutherford Birchard Hayes, 1877-1881\n",
            "James Abram Garfield, 1881-1881\n",
            "Chester Alan Arthur, 1881-1885\n",
            "Grover Cleveland, 1885-1889\n",
            "Benjamin Harrison, 1889-1893\n",
            "Grover Cleveland, 1893-1897\n",
            "William McKinley, 1897-1901\n",
            "Theodore Roosevelt, 1901-1909\n",
            "William Howard Taft, 1909-1913\n",
            "Woodrow Wilson, 1913-1921\n",
            "Warren Gamaliel Harding, 1921-1923\n",
            "Calvin Coolidge, 1923-1929\n",
            "Herbert Clark Hoover, 1929-1933\n",
            "Franklin Delano Roosevelt, 1933-1945\n",
            "Harry S. Truman, 1945-1953\n",
            "Dwight David Eisenhower, 1953-1961\n",
            "John Fitzgerald Kennedy, 1961-1963\n",
            "Lyndon Baines Johnson, 1963-1969\n",
            "Richard Milhous Nixon, 1969-1974\n",
            "Gerald Rudolph Ford, 1974-1977\n",
            "James Earl Carter Jr., 1977-1981\n",
            "Ronald Wilson Reagan, 1981-1989\n",
            "George Herbert Walker Bush, 1989-1993\n",
            "William Jefferson Clinton, 1993-2001\n",
            "George Walker Bush, 2001-2009\n",
            "Barack Hussein Obama, 2009-2017\n",
            "Donald Trump, 2017-2021\n",
            "Joe Biden, 2021-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class President:\n",
        "    def __init__(self, name: str, terms: list):\n",
        "        self.name = name\n",
        "        self.terms =self.__process_term(terms)\n",
        "\n",
        "    def __process_term(self, term: str):\n",
        "      lst = term.split('-')\n",
        "      if lst[1] =='':\n",
        "        start, end = int(lst[0]),datetime.now().year\n",
        "      else:\n",
        "        start, end = int(lst[0]),int(lst[1])\n",
        "      return start,end\n",
        "\n",
        "    def is_served_multiple(self) -> bool:\n",
        "      start, end = self.terms\n",
        "      year = int(end) - int(start)\n",
        "      if year > 4 :\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "RNYIQCYnt9ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oiesRSk4sbFB",
        "outputId": "ae782dd7-bb46-4b3c-b168-7bdc932b34ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_presidents_from_file(file_path: str) -> list:\n",
        "    presidents = []\n",
        "    file_content = open(file_path)\n",
        "    for line in file_content:\n",
        "      name, terms = line.rstrip().split(',')\n",
        "      presidents.append(President(name,terms))\n",
        "    return presidents\n",
        "\n"
      ],
      "metadata": {
        "id": "WchTnVn0eCIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "presidents = read_presidents_from_file('./presidents.txt')\n",
        "count = 0\n",
        "for president in presidents:\n",
        "    if president.is_served_multiple():\n",
        "        count += 1\n",
        "\n",
        "assert count == 18"
      ],
      "metadata": {
        "id": "19HbTZd5XI4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. TextAnalyzer 클래스를 작성하세요. 이 클래스에는 word_frequency() 메서드가 있으며, 생성 시 제공되는 텍스트 파일을 읽고 파일에 있는 각 단어의 빈도를 반환합니다. 단어는 소문자로 변환되어야 하고, 구두점은 무시되어야 합니다. 파일을 찾지 못했을 경우의 예외 처리를 적절히 처리하세요.\n",
        "\n",
        "> 파일 URL: https://gist.github.com/phillipj/4944029\n",
        "\n",
        "\n",
        "# 클래스 TextAnalyzer\n",
        "\n",
        "* 생성자 (init): TextAnalyzer 클래스의 인스턴스를 초기화합니다. 이 생성자는 하나의 인자 file_path를 받으며, 이 인자는 분석할 텍스트 파일의 경로를 나타냅니다. 이 인자는 나중에 다른 메서드에서 사용될 수 있도록 인스턴스 변수로 저장됩니다.\n",
        "\n",
        "> 힌트: 클래스를 인스턴스화할 때, 유효한 파일 경로를 생성자에게 인자로 전달하는지 확인하세요. 이 파일 경로는 word_frequency 메서드에서 사용됩니다.\n",
        "\n",
        "* word_frequency(self): 텍스트 파일을 처리하여 파일 내의 각 단어를 빈도로 매핑하는 딕셔너리를 반환합니다. 이 메서드는 파일의 내용을 읽고, 모든 텍스트를 소문자로 변환하여 대소문자 구분 없는 비교를 보장하며, 구두점을 제거하여 실제 단어에 집중합니다. 그런 다음, 내용을 단어로 분리하고, 각 단어의 출현 빈도를 계산하여 딕셔너리에 저장합니다.\n",
        "\n",
        "> 힌트: 이 메서드는 파일을 읽고 분석하여 각 단어가 몇 번 등장했는지를 카운팅하는 역할을 합니다. 텍스트는 대소문자 구분 없이 처리되어야 하며, 구두점과 같은 불필요한 문자는 제거되어야 합니다. 메서드는 각 단어와 그에 해당하는 빈도를 포함하는 딕셔너리를 반환합니다. 또한, 파일이 존재하지 않을 경우를 처리하는 예외처리도 해야 합니다.\n",
        "\n",
        "> 구두점 제거\n",
        "```python\n",
        "words = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in file_content]).split()\n"
      ],
      "metadata": {
        "id": "mGbPfQJEc5pl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnvxGy3nAjgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self, file_path: str):\n",
        "\n",
        "        self.file_path = Path(file_path)\n",
        "\n",
        "    def word_frequency(self) -> dict:\n",
        "        result = {}\n",
        "        file_content= self.file_path.read_text().lower().rstrip()\n",
        "        words = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in file_content]).split()\n",
        "        # return words\n",
        "        for word in words:\n",
        "          if word in result:\n",
        "            result[word] +=1\n",
        "          else :\n",
        "            result[word] = 1\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "4OXPr0YxXJwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def read_expected_result(expected_result_file):\n",
        "  try:\n",
        "      with open(expected_result_file, 'rb') as f:\n",
        "          expected_result = pickle.load(f)\n",
        "  except FileNotFoundError:\n",
        "      print(\"Expected result file not found.\")\n",
        "      expected_result = None\n",
        "  finally:\n",
        "      return expected_result\n",
        "\n",
        "input_file_path = \"alice_in_wonderland.txt\"\n",
        "expected_result_file = \"expected_result.pkl\"\n",
        "\n",
        "analyzer = TextAnalyzer(input_file_path)\n",
        "result = analyzer.word_frequency()\n",
        "\n",
        "expected_result = read_expected_result(expected_result_file)\n",
        "sorted_result = sorted(result.items(), key=lambda x: (-x[1], x[0]))\n",
        "assert sorted_result == expected_result\n",
        "\n",
        "\n",
        "# missing_file_path = \"missing.txt\"\n",
        "# expected_result_file = \"missing_output.pkl\"\n",
        "# analyzer = TextAnalyzer(missing_file_path)\n",
        "\n",
        "# expected_result = read_expected_result(expected_result_file)\n",
        "# output = analyzer.word_frequency()\n",
        "\n",
        "# assert output == expected_result"
      ],
      "metadata": {
        "id": "trMw_2Fdh97P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 다음을 계산하는 메서드를 포함한 CountryStatistics 클래스를 작성하세요:\n",
        "\n",
        "* GDP가 가장 높은 상위 5개 국가: 이 메서드는 GDP가 가장 높은 상위 5개 국가의 이름을 반환하며, GDP 값은 쉼표로 형식화됩니다.\n",
        "* 기대 수명이 가장 긴 상위 5개 국가: 이 메서드는 기대 수명이 가장 긴 상위 5개 국가의 이름을 반환합니다.\n",
        "* 인구 밀도가 가장 높은 상위 5개 국가: 이 메서드는 인구 밀도가 가장 높은 상위 5개 국가의 이름을 반환합니다.\n",
        "\n",
        "* 파일: https://gist.github.com/vr-23/d6a4a0aadcf3a2640091ca43c25e1955#file-world-data-2023-csv\n",
        "\n",
        "### CountryStatistics 클래스\n",
        "\n",
        "* __clean_numeric_data(value: str): 문자열을 입력으로 받아 쉼표, 달러 기호, 백분율 기호를 제거하고 숫자 값을 float으로 반환합니다. 입력을 변환할 수 없으면 0.0을 반환합니다.\n",
        "\n",
        "> 힌트: 이 함수는 쉼표나 달러 기호가 포함된 CSV 파일의 숫자 데이터를 처리하는 데 유용합니다.\n",
        "빈 문자열이나 숫자가 아닌 값을 처리할 수 있도록 예외 처리에 신경 쓰세요.\n",
        "\n",
        "* __load_data(filename: str): CSV 파일에서 국가 데이터를 로드하고 데이터를 파싱하여 Country 객체를 생성합니다. 정리된 데이터는 self.countries 리스트에 저장됩니다.\n",
        "\n",
        "> 힌트: csv.reader를 사용하여 CSV 파일을 한 줄씩 읽습니다.\n",
        "첫 번째 행은 헤더이므로 건너뜁니다.\n",
        "모든 숫자 데이터 필드에 대해 __clean_numeric_data를 호출하여 데이터를 저장하기 전에 정확하게 정리된 데이터를 확인하세요.\n",
        "GDP, 기대 수명, 인구 등의 열 인덱스를 정확하게 매핑하세요.\n",
        "\n",
        "* top_5_gdp(): GDP가 가장 높은 상위 5개 국가의 이름과 쉼표로 형식화된 GDP 값을 포함한 튜플 목록을 반환합니다.\n",
        "\n",
        "> 힌트: 국가들을 gdp 속성 기준으로 내림차순 정렬하세요.\n",
        "슬라이싱 ([:5])을 사용하여 상위 5개 항목을 선택하세요.\n",
        "국가 이름과 형식화된 GDP 값을 반환하세요.\n",
        "\n",
        "* top_5_life_expectancy(): 기대 수명이 가장 긴 상위 5개 국가의 이름과 그들의 기대 수명 값을 포함한 튜플 목록을 반환합니다.\n",
        "\n",
        "> 힌트: 국가들을 기대 수명(life_expectancy) 기준으로 내림차순 정렬하세요.\n",
        "슬라이싱 ([:5])을 사용하여 상위 5개 항목을 추출하세요.\n",
        "기대 수명 값은 형식화할 필요가 없으므로 있는 그대로 반환하세요.\n",
        "\n",
        "* countries_by_density(): 인구 밀도가 가장 높은 상위 5개 국가의 이름과 인구 밀도를 반환합니다.\n",
        "\n",
        "> 힌트: 국가들을 인구 밀도(density) 속성 기준으로 내림차순 정렬하세요.\n",
        "슬라이싱 ([:5])을 사용하여 상위 5개 항목을 선택하세요.\n",
        "인구 밀도 값은 정수이므로 형식화할 필요가 없습니다.\n"
      ],
      "metadata": {
        "id": "INqFk2K5c7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asa='&fgsfsd$'\n",
        "b=asa.strip('&$')\n",
        "print(b)"
      ],
      "metadata": {
        "id": "5PIYrm1JTVKS",
        "outputId": "af479ce5-7fdc-41fb-eba8-22a43cad6fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fgsfsd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "class Country:\n",
        "    def __init__(self, name: str, density: int, land_area: int, gdp: float, life_expectancy: float, population: int):\n",
        "        self.name = name\n",
        "        self.density = density\n",
        "        self.land_area = land_area\n",
        "        self.gdp = gdp\n",
        "        self.life_expectancy = life_expectancy\n",
        "        self.population = population\n",
        "\n",
        "class CountryStatistics:\n",
        "    def __init__(self, filename: str, info=[]):\n",
        "        self.filename = filename\n",
        "        self.info = self.__load_data(self.filename)\n",
        "\n",
        "    def __clean_numeric_data(self, value: str) -> float:\n",
        "        self.value = value.replace(',','').replace('$','')\n",
        "        return float(self.value)\n",
        "\n",
        "    def __load_data(self, filename: str):\n",
        "        info=[]\n",
        "        with open(self.filename , 'r', encoding='utf-8') as f:\n",
        "           reader = csv.reader(f)\n",
        "           for i in reader:\n",
        "              info.append(i)\n",
        "        return info[1:]\n",
        "\n",
        "    def top_5_gdp(self):\n",
        "        for gdp in self.info:\n",
        "           if gdp[16] == '':\n",
        "             gdp[16] = 0.0\n",
        "           else:\n",
        "            gdp[16]= self.__clean_numeric_data(gdp[16])\n",
        "        self.info.sort(key=lambda x: -x[16])\n",
        "        top5 = []\n",
        "        for i in range(5):\n",
        "         top5.append((self.info[i][0],self.info[i][16]))\n",
        "        return top5\n",
        "\n",
        "\n",
        "\n",
        "    def top_5_life_expectancy(self):\n",
        "       for life in self.info:\n",
        "           if life[21] == '':\n",
        "             life[21] = 0.0\n",
        "           else:\n",
        "            life[21]= self.__clean_numeric_data(life[21])\n",
        "       self.info.sort(key=lambda x: -x[21])\n",
        "       top5 = []\n",
        "       for i in range(5):\n",
        "         top5.append((self.info[i][0],self.info[i][21]))\n",
        "       return top5\n",
        "\n",
        "    def countries_by_density(self):\n",
        "       for countries in self.info:\n",
        "           if countries[1] == '':\n",
        "             countries[1] = 0.0\n",
        "           else:\n",
        "            countries[1]= self.__clean_numeric_data(countries[1])\n",
        "       self.info.sort(key=lambda x: -x[1])\n",
        "       top5 = []\n",
        "       for i in range(5):\n",
        "         top5.append((self.info[i][0],self.info[i][1]))\n",
        "       return top5\n"
      ],
      "metadata": {
        "id": "u-O90MGn4Hmy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expected_gdp_output = [\n",
        "#     (\"United States\", \"21,427,700,000,000\"),\n",
        "#     (\"China\", \"19,910,000,000,000\"),\n",
        "#     (\"Japan\", \"5,081,769,542,380\"),\n",
        "#     (\"Germany\", \"3,845,630,030,824\"),\n",
        "#     (\"United Kingdom\", \"2,827,113,184,696\")\n",
        "# ]\n",
        "\n",
        "expected_gdp_output = [\n",
        "    (\"United States\", 21427700000000),\n",
        "    (\"China\", 19910000000000),\n",
        "    (\"Japan\", 5081769542380),\n",
        "    (\"Germany\", 3845630030824),\n",
        "    (\"United Kingdom\", 2827113184696)\n",
        "]\n",
        "\n",
        "expected_life_expectancy_output = [\n",
        "    (\"San Marino\", 85.4),\n",
        "    (\"Japan\", 84.2),\n",
        "    (\"Switzerland\", 83.6),\n",
        "    (\"Spain\", 83.3),\n",
        "    (\"Singapore\", 83.1)\n",
        "]\n",
        "\n",
        "expected_density_output = [\n",
        "    (\"Monaco\", 26337),\n",
        "    (\"Singapore\", 8358),\n",
        "    (\"Bahrain\", 2239),\n",
        "    (\"Vatican City\", 2003),\n",
        "    (\"Maldives\", 1802)\n",
        "]\n",
        "\n",
        "expected_average_population = 57\n",
        "\n",
        "stats = CountryStatistics('./countries.csv')\n",
        "\n",
        "for (expected_country, expected_gdp), (result_country, result_gdp) in zip(expected_gdp_output, stats.top_5_gdp()):\n",
        "    assert expected_country == result_country\n",
        "    assert expected_gdp == result_gdp\n",
        "\n",
        "for (expected_country, expected_life), (result_country, result_life) in zip(expected_life_expectancy_output, stats.top_5_life_expectancy()):\n",
        "    assert expected_country == result_country\n",
        "    assert expected_life == result_life\n",
        "\n",
        "for (expected_country, expected_density), (result_country, result_density) in zip(expected_density_output, stats.countries_by_density()):\n",
        "    assert expected_country == result_country\n",
        "    assert expected_density == result_density\n"
      ],
      "metadata": {
        "id": "gtu5YK_Vx623"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 파일에서 숫자 데이터를 처리하고 합계, 평균, 최소값, 최대값 및 중앙값과 같은 기본 통계를 계산하는 메서드를 제공하는 FileProcessor 클래스를 작성하세요.\n",
        "\n",
        "### FileProcessor 클래스\n",
        "\n",
        "* 생성자: FileProcessor 객체를 초기화합니다. 파일 이름을 입력으로 받아 저장하고, 이후에 파일에서 읽어온 숫자 데이터를 저장할 빈 리스트 self.data를 초기화합니다.\n",
        "\n",
        "> 힌트: 이 메서드는 파일을 읽고 처리할 준비를 합니다. 파일 경로는 저장되지만, 데이터는 다른 메서드에서 요청할 때까지 로드되지 않습니다.\n",
        "\n",
        "* __read_file(self): 파일에서 데이터를 읽습니다. 파일을 읽기 모드로 열고, 각 줄에서 불필요한 문자를 제거한 후 데이터를 실수로 변환합니다. 이 처리된 데이터를 self.data에 저장합니다.\n",
        "\n",
        "> 힌트: 이 메서드를 사용하여 필요한 경우에만 데이터를 지연 로드합니다. FileNotFoundError 및 ValueError와 같은 파일 관련 오류를 처리하여 파일 읽기가 견고하게 이루어지도록 합니다.\n",
        "\n",
        "* calc_sum(self): 숫자 데이터의 합계를 계산합니다. 데이터가 아직 읽히지 않았다면 먼저 _read_file 메서드를 호출합니다. 데이터가 사용 가능해지면 값을 합산하고 결과를 소수점 한 자리까지 반올림하여 반환합니다.\n",
        "\n",
        "> 힌트: 계산을 수행하기 전에 파일이 읽혔는지 확인하세요. 내장된 sum() 함수를 사용하여 코드를 간소화할 수 있습니다. 빈 파일이 있을 수 있으므로 _read_file이 성공적으로 호출된 경우에만 이 메서드가 제대로 작동합니다.\n",
        "\n",
        "* calc_average(self): 숫자 데이터의 평균(산술 평균)을 계산합니다. 필요한 경우 _read_file을 호출하여 데이터가 사용 가능한지 확인합니다. 데이터가 없을 경우 ZeroDivisionError를 발생시킵니다.\n",
        "\n",
        "> 힌트: 평균은 데이터의 합계를 항목 수로 나눈 값입니다. 나누기를 수행하기 전에 항목이 있는지 확인하여 0으로 나누는 오류를 방지하세요.\n",
        "\n",
        "* calc_min(self): 데이터셋에서 최소값을 계산하고 반환합니다. 아직 파일을 읽지 않았다면 파일을 읽어 데이터를 확인합니다. 데이터가 없을 경우 ValueError를 발생시킵니다.\n",
        "\n",
        "> 힌트: Python의 내장 min() 함수를 사용하여 최소값을 찾습니다. 데이터가 없는 경우 적절하게 예외를 발생시켜야 합니다.\n",
        "\n",
        "* calc_max(self): 데이터셋에서 최대값을 계산하고 반환합니다. 필요한 경우 데이터를 읽고, 데이터가 비어 있으면 ValueError를 발생시킵니다.\n",
        "\n",
        "> 힌트: calc_min()과 유사하게 구현되지만, max()를 사용하여 가장 큰 숫자를 반환합니다.\n",
        "\n",
        "* calc_median(self): 숫자 데이터의 중앙값을 계산하고 반환합니다. 데이터를 정렬한 후 중앙값을 계산합니다. 데이터 포인트 수가 홀수인 경우 중간값을 반환하고, 짝수인 경우 두 중간값의 평균을 반환합니다.\n",
        "\n",
        "> 힌트: 데이터를 읽은 후 정렬하여 중앙값을 계산합니다. 데이터 길이가 짝수일 경우 중앙값은 두 중간 숫자의 평균이고, 홀수일 경우 중앙값은 중간 숫자입니다.\n",
        "\n",
        "### 오류 처리\n",
        "\n",
        "클래스는 파일을 찾을 수 없거나 파일에 유효하지 않은 데이터가 있을 때 적절한 예외를 발생시켜야 합니다.\n",
        "\n",
        "* FileNotFoundError: 파일이 존재하지 않을 때 발생합니다.\n",
        "* ValueError: 파일의 한 줄이 실수로 변환할 수 없거나 최소값, 최대값 또는 중앙값 작업에 대해 빈 파일이 제공될 때 발생합니다.\n",
        "* ZeroDivisionError: 파일이 비어 있어 평균을 계산할 데이터가 없을 때 발생합니다.\n"
      ],
      "metadata": {
        "id": "LtGlz5V2Nhmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FileProcessor:\n",
        "    def __init__(self, file_name: str):\n",
        "        self.file_name = file_name\n",
        "        self.data = []\n",
        "        self.__read_file()\n",
        "\n",
        "    def __read_file(self):\n",
        "        content =\n",
        "\n",
        "    def calc_sum(self) -> float:\n",
        "        pass\n",
        "\n",
        "    def calc_average(self) -> float:\n",
        "        pass\n",
        "\n",
        "    def calc_min(self) -> float:\n",
        "        pass\n",
        "\n",
        "    def calc_max(self) -> float:\n",
        "        pass\n",
        "\n",
        "    def calc_median(self) -> float:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "2UAF4rYtNqtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_files = ['./Numbers.txt', './EmptyFile.txt', './InvalidFile.txt']\n",
        "expected_sums = [23759.0, None, None]\n",
        "expected_averages = [475.2, None, None]\n",
        "expected_mins = [33.0, None, None]\n",
        "expected_maxs = [948.0, None, None]\n",
        "expected_medians = [442.5, None, None]\n",
        "\n",
        "failed_tests = []\n",
        "\n",
        "for file, expected_sum, expected_avg, expected_min, expected_max, expected_median in zip(input_files, expected_sums, expected_averages, expected_mins, expected_maxs, expected_medians):\n",
        "    processor = FileProcessor(file)\n",
        "\n",
        "    try:\n",
        "        result_sum = processor.calc_sum()\n",
        "        if expected_sum is not None:\n",
        "            assert round(result_sum, 1) == round(expected_sum, 1)\n",
        "    except (FileNotFoundError, ValueError, ZeroDivisionError) as e:\n",
        "        if expected_sum is None:\n",
        "            pass\n",
        "        else:\n",
        "            failed_tests.append(f\"Sum test failed for {file}: {e}\")\n",
        "\n",
        "    try:\n",
        "        result_avg = processor.calc_average()\n",
        "        if expected_avg is not None:\n",
        "            assert round(result_avg, 1) == round(expected_avg, 1)\n",
        "    except (FileNotFoundError, ValueError, ZeroDivisionError) as e:\n",
        "        if expected_avg is None:\n",
        "            pass\n",
        "        else:\n",
        "            failed_tests.append(f\"Average test failed for {file}: {e}\")\n",
        "\n",
        "    try:\n",
        "        result_min = processor.calc_min()\n",
        "        if expected_min is not None:\n",
        "            assert round(result_min, 1) == round(expected_min, 1)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        if expected_min is None:\n",
        "            pass\n",
        "        else:\n",
        "            failed_tests.append(f\"Min test failed for {file}: {e}\")\n",
        "\n",
        "    try:\n",
        "        result_max = processor.calc_max()\n",
        "        if expected_max is not None:\n",
        "            assert round(result_max, 1) == round(expected_max, 1)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        if expected_max is None:\n",
        "            pass\n",
        "        else:\n",
        "            failed_tests.append(f\"Max test failed for {file}: {e}\")\n",
        "\n",
        "    try:\n",
        "        result_median = processor.calc_median()\n",
        "        if expected_median is not None:\n",
        "            assert round(result_median, 1) == round(expected_median, 1)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        if expected_median is None:\n",
        "            pass\n",
        "        else:\n",
        "            failed_tests.append(f\"Median test failed for {file}: {e}\")\n",
        "\n",
        "if failed_tests:\n",
        "    for test in failed_tests:\n",
        "        print(test)"
      ],
      "metadata": {
        "id": "8HB7gKAQOZs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}